<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Document Summarization :: LangChain AI</title>
    <link>http://localhost:1313/langchain_project_book/doc_sum/index.html</link>
    <description>In the previous chapters, we discussed the key components and features of LangChain, along with instructions on setting up a development environment. Our next step involves integrating these elements into a practical case study.&#xA;Business Scenario Let’s say that there are numerous interconnected documents, sharing similar narratives within a specific domain such as sales strategies or human resource policies. The goal is to develop a bot capable of comprehending these documents and responding to user queries based on the information contained within. All data, queries, responses, and computational processes will be confined within an internal network to ensure data security compliance within the enterprise. Additionally, the outcomes of user queries and the bot’s responses can be stored for potential future analytics purposes.</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 28 Oct 2024 21:13:34 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/langchain_project_book/doc_sum/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Software Environment</title>
      <link>http://localhost:1313/langchain_project_book/doc_sum/software_env/index.html</link>
      <pubDate>Mon, 28 Oct 2024 20:58:01 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/doc_sum/software_env/index.html</guid>
      <description>Python As LangChain, introduced in this book, is based on Python, the following is the requirements file containing the necessary libraries and modules for LangChain installation.&#xA;In my development environment, I have the following libraries installed:&#xA;Reminder: The community is evolving, and the library is adapting rapidly. The information presented here may change over time. This list serves as a reference for the current chapter being written.&#xA;beautifulsoup4 4.12.3 faiss-cpu 1.9.0 huggingface-hub 0.26.2 langchain 0.3.7 langchain-chroma 0.1.4 langchain-community 0.3.5 langchain-core 0.3.15 langchain-experimental 0.3.3 langchain-huggingface 0.1.2 langchain-ollama 0.2.0 langchain-openai 0.2.6 langchain-qdrant 0.2.0 langchain-text-splitters 0.3.2 numpy 1.26.4 pypdf 5.1.0 requests 2.32.3 requests-oauthlib 2.0.0 requests-toolbelt 1.0.0 sentence-transformers 3.2.1 torch 2.5.1 transformers 4.46.2 Architecture Let’s observe the architectural overview:</description>
    </item>
    <item>
      <title>Setting up Vectorstore with Qdrant</title>
      <link>http://localhost:1313/langchain_project_book/doc_sum/setup_vectorstore_with_qdrant/index.html</link>
      <pubDate>Mon, 28 Oct 2024 21:06:19 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/doc_sum/setup_vectorstore_with_qdrant/index.html</guid>
      <description>A vector database is a specialized database created to transform data, typically textual data, into multi-dimensional vectors, also referred to as vector embeddings, and store them systematically. These vectors serve as mathematical representations of attributes or features. The dimensionality of each vector can vary significantly, ranging from a few dimensions to several thousand, depending on the complexity and granularity of the data.&#xA;In this book, we’re going to cover the 4 open-source vector databases: Qdrant, FAISS, Supabase and Chroma. They’re standard and straight-forward. An important consideration is the necessity of consistently utilizing the same embedding model for both embedding the data into VectorStore and conducting similarity searches within VectorStore at a later stage.</description>
    </item>
    <item>
      <title>Define a RAG Chain</title>
      <link>http://localhost:1313/langchain_project_book/doc_sum/define_a_rag_chain/index.html</link>
      <pubDate>Mon, 28 Oct 2024 21:09:57 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/doc_sum/define_a_rag_chain/index.html</guid>
      <description>Using PyPDFLoader and DirectoryLoader to load the multiple PDF documents from a specific directory The example provided utilizes two components:&#xA;PyPDFLoader: This tool is used to load and parse PDF documents. DirectoryLoader: This component enables the simultaneous processing of multiple PDF files from a specific directory. As an example, the following PDF document was randomly downloaded: https://ufdcimages.uflib.ufl.edu/AA/00/01/16/99/00001/WorldHistory.pdf&#xA;Copy this file to your project or any directory you like. In my case, I leave it in /tmp. This PDF file will be used to demonstrate the functionality of the PyPDFLoader and DirectoryLoader tools.</description>
    </item>
    <item>
      <title>Summary</title>
      <link>http://localhost:1313/langchain_project_book/doc_sum/summary/index.html</link>
      <pubDate>Mon, 28 Oct 2024 21:13:34 +0800</pubDate>
      <guid>http://localhost:1313/langchain_project_book/doc_sum/summary/index.html</guid>
      <description>This chapter covers several skills related to LangChain Retrieval Augmented Generation (RAG) with the open source LLM. Here are some of the skills you can learn from this chapter:&#xA;Run a VectorStore in Docker mode. Define a VectorStore as a query-only client. Perform a full RAG to summarize an entire document from one or more PDF files. Launch Mistral LLM with Ollama. Execute everything locally using the open source LLM, without exposing any data on internet, if you have sensitive data, such as your client and business information. Set up debug and verbose modes to observe the detailed chain of events.</description>
    </item>
  </channel>
</rss>