var relearn_searchindex = [
  {
    "breadcrumb": "LangChain Project Handbook \u003e Tools and Libraries",
    "content": "Install LangChain To install LangChain, you can use pip or conda.\nTo install using pip, run the command pip install langchain To install using conda, run the command conda install langchain -c conda-forge It’s always recommended to check the latest version of LangChain at https://github.com/langchain-ai/langchain\nAt the point of writing this book, the version of LangChain that I’m using is 0.1.11\npip list | grep langchain langchain 0.3.4 langchain-chroma 0.1.2 langchain-community 0.2.1 langchain-core 0.3.13 langchain-experimental 0.0.59 langchain-huggingface 0.1.0 langchain-openai 0.2.3 langchain-text-splitters 0.3.0 Install the required Python packages associated with your chosen LLM providers As we intend to utilize open-source language models from Hugging Face platform within LangChain, it is necessary to configure Hugging Face accordingly. Execute the following command:\npip install huggingface-hub In my development environment, I use the following main libraries to work on models. By the way, I attach a requirements.txt at the end of this chapter for reference.\npip list | grep -i 'faiss\\|huggingface\\|langchain\\|transformers\\|openai\\|qdrant\\|tensor\\|torch\\|tokenizers\\|tiktoken' faiss-cpu 1.8.0 huggingface-hub 0.23.2 langchain 0.2.11 langchain-chroma 0.1.2 langchain-community 0.2.1 langchain-core 0.2.25 langchain-experimental 0.0.59 langchain-huggingface 0.0.3 langchain-openai 0.1.8 langchain-text-splitters 0.2.0 openai 1.31.1 safetensors 0.4.3 sentence-transformers 3.0.0 tiktoken 0.7.0 tokenizers 0.19.1 torch 2.3.0 transformers 4.41.2",
    "description": "Install LangChain To install LangChain, you can use pip or conda.\nTo install using pip, run the command pip install langchain To install using conda, run the command conda install langchain -c conda-forge It’s always recommended to check the latest version of LangChain at https://github.com/langchain-ai/langchain\nAt the point of writing this book, the version of LangChain that I’m using is 0.1.11\npip list | grep langchain langchain 0.3.4 langchain-chroma 0.1.2 langchain-community 0.2.1 langchain-core 0.3.13 langchain-experimental 0.0.59 langchain-huggingface 0.1.0 langchain-openai 0.2.3 langchain-text-splitters 0.3.0 Install the required Python packages associated with your chosen LLM providers As we intend to utilize open-source language models from Hugging Face platform within LangChain, it is necessary to configure Hugging Face accordingly. Execute the following command:",
    "tags": [],
    "title": "Install LangChain",
    "uri": "/langchain_project_book/tools_n_lib/install_lc/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e LangChain Fundamentals",
    "content": "The architectural components of LangChain, illustrated in Figure 1.1, will be thoroughly explored and discussed in detail throughout the book.\npackage LangChain { package Agent_Tooling { agent Tools agent Toolkits } package Models_IO { agent Model agent Prompt agent Output_Parser } package Chain_and_Retrieval { agent Retriever agent Document_Loaders agent VectorStore agent TextSplitter agent Embedding_Model } } Chain_and_Retrieval -[hidden] Models_IO Models_IO -[hidden] Agent_Tooling Model -[hidden]- Prompt Prompt -[hidden]- Output_Parser Retriever -[hidden] Document_Loaders Document_Loaders -[hidden]- VectorStore VectorStore -[hidden] TextSplitter TextSplitter -[hidden]- Embedding_Model Tools -[hidden]- Toolkits Figure 1.1: LangChain Architecture\nModel: also known as LLM model serve as the core elements of LangChain. They essentially act as wrappers for these models, enabling the utilization of their specific functionalities and capabilities. Chain: Chain enables us to integrate multiple components to address a specific task. It streamlines the process of implementing complex applications by enhancing modularity, making debugging and maintenance more straightforward. Document loaders: Document Loaders play a role in loading documents into the LangChain system, managing a range of document types like PDFs, and transforming them into a compatible format for LangChain processing. This procedure encompasses multiple stages, including data ingestion, context comprehension, and refinement. Prompt: Prompts serve as inputs for LLMs to generate specific responses. Crafting effective prompts is vital for obtaining valuable outputs from LLMs. Prompt engineering aims to create prompts that yield precise, relevant, and beneficial responses from LLMs. For instance, the prompt plays an amazing role in the output when examining the prompt in OpenAI’s Sora, which creates stunning and visually striking videos. VectorStore: It brings functions for the effective storage and retrieval of vector embeddings. And it operates as a repository for vectors containing supplementary data, streamlining the processes of storage, search, and point management. Output Parsers: The output_parser converts the output of an LLM into a more appropriate format, especially beneficial when generating structured data using LLMs. Agents: LLMs can communicate with their surroundings through agents. For instance, carrying out a particular task via an external API, or grabbing extra data from outside website. LangChain utilizes a sequential pipeline method to construct tailored applications for LLM. This structured approach integrates diverse services, data inputs, and formatting processes, ensuring accurate processing and consistent output. Modules in LangChain follow a step-by-step process with single inputs and outputs, facilitating smooth data flow. This mechanism simplifies development and enhances LLM utilization. By streamlining workflows, LangChain optimizes AI application development, executing steps in a specific order to real-world processes for managed outcomes.\nLangChain Workflow Having grasped the fundamental elements of LangChain, let’s observe its process in detail and how the message is handled. The actual scenarios can change the workflow’s logic depending on the requirements. A very common conversation flow is shown in Figure 1.2, which includes document_loaders, data embedding into vectorstore, and query similarity_search within RetrievalQA chain, then returns the analyzed result to the user.\nactor user component Load_Docs component Query component LLM_generates_answer package LangChain { component Document_Loaders component CharacterTextSplitter component embeddings component PromptTemplate component RetrievalQA } Load_Docs -\u003e Document_Loaders user -\u003e Query Query -\u003e RetrievalQA Document_Loaders -\u003e CharacterTextSplitter CharacterTextSplitter --\u003e embeddings PromptTemplate \u003c- embeddings PromptTemplate --\u003e RetrievalQA RetrievalQA -\u003e LLM_generates_answer Load_Docs -[hidden]- user Query -[hidden]- LLM_generates_answer Figure 1.2: LangChain Workflow\nLet’s discuss these steps in detail. I’ve included the Python code for a set of typical modules to demonstrate the components. The real projects will be covered in detail in the upcoming chapters. To run the Python code provided, it is required to set up a working environment, a procedure that will be elaborated on in the upcoming chapter. You can directly proceed to the next chapter to configure your development environment, ensuring the necessary libraries are installed and properly set up for the successful execution of the sample code presented here.\ndocument_loaders can load, extract data from diverse sources and transform it into structured documents. It can handle *.txt (plain text) and *.xls (Microsoft Excel), load the HTML content from any website. Here’s an example of loading data from Wikipedia through WebBaseLoader\nfrom langchain_community.document_loaders import WebBaseLoader loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Text_file\") document = loader.load() LangChain uses the TextSplitter class to break down the document into smaller chunks that can be more easily processed by the language model. One of the most used splitters, RecursiveCharacterTextSplitter, divides a large text into chunks based on a defined size, using a set of characters. By default, it utilizes characters like [\"\\n\\n\", \"\\n\", \" \"], and [\"\"]. Initially, it attempts to split the text using “\\n\\n”. If the resulting chunks are still too large, it progresses to the next character, “\\n”, for further splitting. This process continues through the set of characters until a split smaller than the specified chunk size is achieved. The chunk_size parameter controls the max size of the final documents, and the chunk_overlap parameter specifies how much overlap there should be between chunks.\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50) docs = splitter.split_documents(document) LangChain uses a VectorStore to create embeddings for each document split. These embeddings are numerical representations of the text that can be used for efficient information retrieval. The provided code snippet utilizes the all-mpnet-base-v2 model from HuggingFaceEmbeddings by default. If not explicitly specified, this model is used. Additionally, the code operates with a vector store based on Qdrant, running in memory. (Please be aware that the vector store we have just set up operates in memory, which implies that all data will be lost when your computer is turned off. The advantage of utilizing a memory-based vector store is the ability to swiftly test your code without the need to save it. We will delve into persistent storage for production purposes in upcoming chapters). The collection is named wikipedia in vectorstore.\nfrom langchain_community.embeddings import HuggingFaceEmbeddings from langchain_community.vectorstores import Qdrant vectorstore = Qdrant.from_documents( docs, embedding=HuggingFaceEmbeddings(), location=\":memory:\", collection_name=\"wikipedia\", ) # You can print out all parameters of Embeddings # print(HuggingFaceEmbeddings()) Subsequently, the VectorStore is employed to conduct a similarity_search on the document embeddings, aiming to identify the documents most pertinent to the user’s query. The search provides relevance scores for the query and outputs 2 results.\nquery = \"What's flatfile?\" result = vectorstore.similarity_search_with_score(query, k=2) print(result) To make foundation models useful for domain-specific tasks, the Retrieval Augmented Generation (RAG) framework augments prompts with external data from multiple sources, including document repositories, databases, or APIs. In a later chapter, we will examine how RAG functions in actual projects. For the time being, this is a brief excerpt of RAG code that illustrates how retrieval functions to obtain data from vectorstore using meaning rather than keywords.\nquery = \"What is flatfile?\" retriever = vectorstore.as_retriever() print(retriever.get_relevant_documents(query)[0]) The entire code, for instance, looks like this:\nfrom langchain_community.document_loaders import WebBaseLoader loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Text_file\") document = loader.load() from langchain.text_splitter import RecursiveCharacterTextSplitter splitter = RecursiveCharacterTextSplitter( chunk_size=1000, chunk_overlap=50 ) docs = splitter.split_documents(document) from langchain_community.embeddings import HuggingFaceEmbeddings from langchain_community.vectorstores import Qdrant vectorstore = Qdrant.from_documents( docs, embedding=HuggingFaceEmbeddings(), location=\":memory:\", collection_name=\"wikipedia\", ) # print(HuggingFaceEmbeddings()) query = \"What's flatfile?\" result = vectorstore.similarity_search_with_score(query, k=2) print(result) retriever = vectorstore.as_retriever() print(retriever.get_relevant_documents(query)[0]) The above code snippet goes through the following steps to process the document\nLoad a document, and split it into smaller chunks Insert the splitted chunks into VectoreStore, which is configured as running in memory. You may want to persist it in production Create a query, then send it to VectorStore for similarity_search Retrieve the relevant document The flow is straight forward and simple. The code in the previous example follows a fundamental flow, incorporating VectorStore with embeddings but not yet involving LLM. Our next step is to introduce an LLM into the process.",
    "description": "The architectural components of LangChain, illustrated in Figure 1.1, will be thoroughly explored and discussed in detail throughout the book.\npackage LangChain { package Agent_Tooling { agent Tools agent Toolkits } package Models_IO { agent Model agent Prompt agent Output_Parser } package Chain_and_Retrieval { agent Retriever agent Document_Loaders agent VectorStore agent TextSplitter agent Embedding_Model } } Chain_and_Retrieval -[hidden] Models_IO Models_IO -[hidden] Agent_Tooling Model -[hidden]- Prompt Prompt -[hidden]- Output_Parser Retriever -[hidden] Document_Loaders Document_Loaders -[hidden]- VectorStore VectorStore -[hidden] TextSplitter TextSplitter -[hidden]- Embedding_Model Tools -[hidden]- Toolkits Figure 1.1: LangChain Architecture",
    "tags": [],
    "title": "LangChain Architecture",
    "uri": "/langchain_project_book/fundamentals/architecture/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook",
    "content": "It is with great pleasure and enthusiasm that I present to you this book on the orchestration of large language models with LangChain. As an experienced Python and LangChain developer, I have had the privilege of participating in numerous projects that revolve around language modeling. These engagements have provided me with invaluable hands-on experience and insight into the complexities and challenges associated with building and managing large language models.\nThe goal of this book is to equip you, the reader, with the knowledge and skills necessary to successfully orchestrate large language models using LangChain. We will explore the intricacies of language modeling, delve into the nuances of LangChain, and provide practical guidance on how to effectively manage and optimize language models at scale. This book aims to serve as your comprehensive guide, blending theory with real-world scenarios to offer a holistic understanding of this cutting-edge technology.\nFurthermore, my passion for open-source software development has led me to contribute to the open-source community for the past two decades. With this book, I not only intend to provide valuable insights but also contribute to the rapidly expanding pool of knowledge and resources available to the open-source software community. It is my hope that by sharing my experiences and expertise, we can collectively advance the field of language modeling and empower others to build upon our work.\nFinally, I encourage you, the reader, to embark on this journey through the pages of this book with an open mind and a thirst for knowledge. I hope that the information presented here will empower you to leverage LangChain’s capabilities to orchestrate and optimize large language models, enabling you to bring your own language model projects to new heights of success.\nThank you for embarking on this adventure, and may your exploration of LangChain and large language models be rewarding and fruitful.\n“If I have seen further, it is by standing upon the shoulders of giants.\"\nApproach This book offers a comprehensive understanding of LangChain, including its core concepts, real-world use cases, and GitHub code examples. Readers will confidently orchestrate language models with LangChain for advanced natural language processing.\nShort Description Discover LangChain’s functions, design insights, and real-world applications like retrieval augmented generation. Engage with the vibrant LangChain open-source community to unlock its potential for powerful language model applications.\nLong Description In the rapidly evolving world of technology, LangChain emerges as a game-changer. In this book, you will discover LangChain’s importance in the tech world and delve into its functions for creating advanced language model applications. This book equips you with the knowledge to construct context-aware applications that enable language models to interact with their environment and other data sources. The book gives you a hands-on practice to build four applications using LangChain. Throughout the book, you will learn to enhance data processing in four project. In “Book Summarization and Q\u0026A - Project One,” LangChain facilitates the management of private data, while “Ticketing System - Project Two” streamlines customer support ticket handling through semantic analysis. “Knowledge Base Semantic Analysis - Project Three” employs LangChain for efficient similarity search and semantic analysis in a knowledge base. Lastly, “Intelligent Programming Assistant - Project Four” harnesses the power of LangChain to generate code and natural language from code and text prompts, offering support for multiple programming languages. By the end of this book, you’ll acquire the expertise to create LLM apps with LangChain, from Python setup to model integration, and become proficient in creating custom language model applications for various domains.\nIn the rapidly evolving world of technology, LangChain emerges as a game-changer. In this book, you will discover LangChain’s importance in the tech world and delve into its functions for creating advanced language model applications.\nThis book equips you with the knowledge to construct context-aware applications that enable language models to interact with their environment and other data sources. The book gives you a hands-on practice to build four applications using LangChain. For instance, in the first application, titled “Book Summarization and Q\u0026A - Project One,” we utilize LangChain to orchestrate the processing of private data in a specific domain with an open source language model.\nIn “Ticketing System - Project Two,” LangChain orchestrates the processing of customer support tickets within a private network using domain-specific data. The system automates ticket handling, streamlining customer support, and improving response times and accuracy through semantic analysis performed by a Large Language Model.\nIn “Knowledge Base Semantic Analysis - Project Three,” - a knowledge base is loaded into a vector database. LangChain provides task scheduling, data management, and fault tolerance features to orchestrate the process of similarity search. The LLM then performs semantic analysis on queries, identifying the most relevant information in the knowledge base.\nIn “Intelligent Programming Assistant - Project Four” - this LLM generates code and natural language about code, from both code and natural language prompts. It can also be used for code completion and debugging. It supports many popular programming languages including Python\nBy the end of this book, you’ll acquire the expertise to create LLM apps with LangChain, from Python setup to model integration, and become proficient in creating custom language model applications for various domains.\nWhat will you learn Begin by introducing LangChain, its history, motivations, and practical applications Dive into LangChain’s core principles, architecture, and how language models interact hierarchically Cover essential components: model training, data management, architecture, and tuning Start with LangChain setup, progress to deployment, including data prep, training, and assessment Apply LangChain to NLP, translation, chatbots, code gen, with real-world examples Explore LangChain’s future through research, projects, societal impact, for insightful contributions Audience This book is primarily targeted towards software developers, machine learning engineers, and AI researchers who wish to understand the intricacies of orchestrating large language models using LangChain. Familiarity with Python programming and basic concepts of machine learning will be beneficial, although not mandatory, as this book will guide you through the fundamentals before delving into the more advanced topics. Additionally, data scientists and language processing enthusiasts looking to leverage language models and explore cutting-edge techniques will find this book valuable.\nAuthor Bio - Jeff Jie Yang The author is an ardent expert in Linux and open-source technologies, with a career spanning two decades. Starting at IBM’s software development labs in Canada, the USA, and China, he transitioned from a software engineer to a Senior Technical Staff Member. His expertise extends to designing architectures using Kubernetes and containerized GitOps, and automation, aligning with standards from the Cloud Native Computing Foundation.\nIn recent 5 years, the author has developed a profound interest in Natural Language Processing, Machine Learning, and Python. This curiosity led him to explore the world of large language models, particularly LangChain. He’s leading a lab utilizing LangChain for several projects, reflecting his technical proficiency and dedication to innovation.",
    "description": "It is with great pleasure and enthusiasm that I present to you this book on the orchestration of large language models with LangChain. As an experienced Python and LangChain developer, I have had the privilege of participating in numerous projects that revolve around language modeling. These engagements have provided me with invaluable hands-on experience and insight into the complexities and challenges associated with building and managing large language models.\nThe goal of this book is to equip you, the reader, with the knowledge and skills necessary to successfully orchestrate large language models using LangChain. We will explore the intricacies of language modeling, delve into the nuances of LangChain, and provide practical guidance on how to effectively manage and optimize language models at scale. This book aims to serve as your comprehensive guide, blending theory with real-world scenarios to offer a holistic understanding of this cutting-edge technology.",
    "tags": [],
    "title": "Preface",
    "uri": "/langchain_project_book/preface/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e Tools and Libraries",
    "content": "In LangChain, when accessing a model from a remote platform, it is necessary to provide an API token (aka access token). For example, as demonstrated in the previous chapter using HuggingFaceEndpoint to utilize mistralai/Mistral-7B-Instruct-v0.2 model from Hugging Face platform, you will be required to generate a HuggingFace access token by following up the instruction at https://huggingface.co/docs/hub/en/security-tokens . You will be able to find your access token at https://huggingface.co/settings/tokens .\nAfter generating the access token, you can proceed to set up and configure the token within the environment based on the following options.\nSet the environment variable To set the environment variable in the terminal, consider adding the following to ~/.bashrc or a similar file. This approach ensures the access token is available for all projects within your environment. Additionally, I will provide guidance on configuring both the OpenAI API key and Hugging Face’s API token.\nexport OPENAI_API_KEY=\"sk-YOUR_API_KEY\" export HUGGINGFACEHUB_API_TOKEN=\"hf_YOUR_API_TOKEN\" My preferred way for storing API token is to use environment variables. This way offers both convenience and security outside of your source code. Storing API token directly in code is not recommended due to potential security risks.\nImportant note:\nIt is important to avoid exposing your any secret key to version control, as unauthorized individuals could potentially impersonate you and gain access to your resources on remote platforms.\nYou also have the option to store your access token in a .env file located within your Python code directory. The .env file should look like the following:\nHUGGINGFACEHUB_API_TOKEN=\"hf_YOUR_API_TOKEN\" Your HUGGINGFACEHUB_API_TOKEN is also stored at ~/.cache/huggingface/token, on Linux system.\nIn Jupyter notebook or Python code Additionally, you can do this from inside the Jupyter notebook or Python code\nimport os os.environ[\"OPENAI_API_KEY\"] = \"sk-YOUR_API_KEY\" os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_YOUR_API_TOKEN\" Or\nfrom langchain.llms import OpenAI llm = OpenAI(openai_api_key=\"OPENAI_API_KEY\") Important note\nWhen pushing code to Github or Gitlab, it is important to prevent sensitive information, such as API keys in .env, from being pushed in public. One way to do this is by setting up a .gitignore file.\nTo enhance your understanding of security measures, I suggest dedicating time to explore the following websites.\nhttps://huggingface.co/docs/hub/security-tokens https://platform.openai.com/docs/quickstart?context=python\nCache setup If you have enough computing resources and opt to execute all tasks locally, including loading the LLM and smaller embedding models, all downloaded models will be stored in the default directory on Linux, at ~/.cache/huggingface/hub/.\nThe default directory can be altered either HF_HOME or HF_HUB_CACHE environment variable. Example for Python:\nimport os os.environ['HF_HOME'] = '/PATH/cache/' And example for Bash on Linux: export HF_HOME=/PATH/cache/ At the same time, PyTorch models like sentence-transformers will be stored in the directory ~/.cache/torch.\nHere is an example on my machine with several LLMs installed.\n(gpt) [jeff@manjaro hub]$ pwd /home/jeff/.cache/huggingface/hub (gpt) [jeff@manjaro hub]$ ls codellama-7b-python.Q4_0.gguf gpt4all models--google-bert--bert-base-chinese models--google--flan-t5-small models--gpt2 models--Salesforce--blip-image-captioning-large (gpt) [jeff@manjaro hub]$ du -ksh 13G . This is the location where the embedding and LLM models are downloaded and installed locally. At times, LLM models like Mistral and Falcon can be quite large, with each potentially exceeding 10GB in size. Consequently, this folder may significantly expand in size, potentially consuming a considerable amount of storage space. It is advisable to monitor the folder’s space periodically to manage storage efficiently.",
    "description": "In LangChain, when accessing a model from a remote platform, it is necessary to provide an API token (aka access token). For example, as demonstrated in the previous chapter using HuggingFaceEndpoint to utilize mistralai/Mistral-7B-Instruct-v0.2 model from Hugging Face platform, you will be required to generate a HuggingFace access token by following up the instruction at https://huggingface.co/docs/hub/en/security-tokens . You will be able to find your access token at https://huggingface.co/settings/tokens .\nAfter generating the access token, you can proceed to set up and configure the token within the environment based on the following options.",
    "tags": [],
    "title": "Set up your Environment",
    "uri": "/langchain_project_book/tools_n_lib/setup_env/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook",
    "content": "The fundamental ideas and elements of LangChain, a framework for creating language-model-powered applications, are covered in this chapter. LangChain aims to develop data-aware and agentic applications that enable language models to communicate with their surroundings and establish connections with other data sources, rather than only calling out to a language model via an API.\nAt the heart of LangChain are two key components. The first one is the modular abstractions provided by LangChain, which are essential for working with language models. These components are designed to be easy to use, whether you use the rest of the LangChain framework or not.\nThe Use-Case Specific Chains make up the second part. These might be conceptualized as putting together the required parts in a certain order to complete a certain use case. These chains are intended to serve as a more advanced interface that makes it simple for users to begin a particular use case. Additionally, they are adaptable, giving you flexibility according to the requirements of the application.\nMoreover, LangChain is context-aware, allowing applications to make decisions depending on the context that is supplied by linking a language model to context-giving sources. Because of this, LangChain is a crucial tool for creating apps that can communicate with language models and make deft choices depending on the environment.\nFrom the high level of LangChain framework, the main components include:\nArchitecture and Workflow Modules and Models Embeddings and VectorStor Chains and Retrievers Technical Requirements To grasp “LangChain Fundamentals”, you’ll need proficiency in Python, including its data science libraries, and a kind of understanding of large language models like LLaMA, Mistral and Gemma. Knowledge of data structures is essential for efficient data processing. Familiarity with machine learning and natural language processing concepts will be beneficial, given LangChain’s focus on language models. Basic understanding of APIs is required for data interactions.\nIn practical projects, deploying a large language model (LLM) as a service typically involves launching it in a Docker container, or Kubernetes. Hence, having familiarity with containers would also be advantageous.\nLangChain is an open-source tool with a Python and JavaScript codebase. However, the examples provided in this guide exclusively focus on Python.\nWhat is LangChain? Imagine a scenario where you have many business reports and you’re scheduled to have a business meeting with your higher management next Monday. However, you have yet to create a summary of these reports. Wouldn’t it be convenient if you could simply ask your text reports to highlight the key points you need to discuss? LangChain is designed to make this task a reality.\nLangChain is an open-source framework designed to help build applications powered by LLMs, like ChatGPT, and create more advanced use cases around LLMs by chaining together different components from several modules.\nLet’s look at some of the features of LangChain:\nIt is an innovative technology that aims to bridge the gap between languages and facilitate communication on a global scale. It leverages artificial intelligence (AI), machine learning (ML), and natural language processing (NLP) to enable real-time translation and interpretation services, not only human languages, but also programming languages. It can be used to develop intelligent applications, such as chatbots, semantic optimization, text generation and summarization, and programming language translation. It offers endless possibilities for creating powerful applications that harness the capabilities of LLMs. It is an open-source framework that streamlines the development of applications utilizing large language models like OpenAI or Hugging Face. It offers end-to-end chains integration to facilitate working with various programming languages, platforms, and data sources. By leveraging these features, developers can create powerful and innovative applications that leverage the power of language models.\nLangChain is not without its challenges, and there are concerns regarding privacy, accuracy, and potential biases in LangChain. We’ll discuss this later in the book.\nWhat Problem does LangChain Resolve? The goal of LangChain is to address the different issues that arise while developing applications with LLMs. Important problems that LangChain aims to solve include:\nStandardizing prompt structures: By making the prompt structure simpler, LangChain helps developers who collaborate with LLMs to work more efficiently. Innovative open-source projects like Lepton and AutoPrompt, accessible on GitHub, focus to standardize prompt structure metadata to address the issue of Language Models struggling to comprehend prompts. Strengthening integration capabilities: LangChain guarantees the smooth application of LLM outputs among various modules and libraries. Simplifying model switching: With LangChain, developers may quickly move between different LLM models, as well as embedding models for VectorStore, in their applications. I am particularly fond of Quivr, a concept known as the “second brain”, which streamlines prompt editing, facilitates model transitions, connects the front-end with the back-end, and supports ongoing conversations while ensuring user access authentication. Similar services can be discovered on platforms such as Phind.com, Perplexity.ai, and Poe.com, among others. Effective memory management: LangChain helps maintain memory records, e.g. follow-up conversation, as needed while developing applications. Optimizing data handling: LangChain improves overall efficiency by streamlining data management in LLM-driven systems. With its framework, LangChain improves the development process with LLMs by providing tools and features that are tailored to address the typical obstacles that developers face while working with AI and data engineering.",
    "description": "The fundamental ideas and elements of LangChain, a framework for creating language-model-powered applications, are covered in this chapter. LangChain aims to develop data-aware and agentic applications that enable language models to communicate with their surroundings and establish connections with other data sources, rather than only calling out to a language model via an API.\nAt the heart of LangChain are two key components. The first one is the modular abstractions provided by LangChain, which are essential for working with language models. These components are designed to be easy to use, whether you use the rest of the LangChain framework or not.",
    "tags": [],
    "title": "LangChain Fundamentals",
    "uri": "/langchain_project_book/fundamentals/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e LangChain Fundamentals",
    "content": "When selecting a language model and an embedding model in LangChain technology, there are several things to consider:\nPrimary Task: Identify the core functions of the language model (LLM), which include tasks like text generation, summarization, translation, and answering queries. A valuable resource to explore these tasks is https://huggingface.co/models covering a wide range from Multimodal to Computer Vision (CV) and Natural Language Processing (NLP). Common examples in this context involve Summarization, Text Generation, and Question Answering within NLP.\nModel Size: The size of LLMs can vary significantly, with models ranging from millions to billions of parameters. Larger models typically offer better performance but are also more computationally expensive.\nPre-Trained vs. Fine-Tuned: Fine-tuned models are designed specifically for a given task or domain, whereas pre-trained models are appropriate for a variety of tasks. Based on my practical experience, fine-tuning a custom model in a real project is not as straightforward as anticipated in theory. The objective of fine-tuning is to train a customized model using a base model like Llama2. The main challenge lies in the difficulty of curating a dataset of sufficient quality and quantity, which is not a simple work, to effectively train a domain-specific model. One of my projects entailed training a renowned novel (the dataset comprises all content of the novel and numerous comments carefully chosen for their guaranteed quality) using multiple base models, notably llama2. We dedicated four weeks to curate up to 10,000 question-answer pairs (not enough obviously) before inputting them into the base model for training. Due to the novel-specific nature of these QA pairs, ensuring their accuracy and quality through thorough review proved to be quite time-consuming with a certain level of knowledge required. Unfortunately, the model’s performance post-training fell short of expectations, likely due to our supplied data being buried within the generic framework of the base model, which is typically tailored for broader applications rather than domain-specific tasks. Instead of fine-tuning your own model, I would suggest considering retrieval augmented generation (RAG) due to its advantages in query quality, performance, and task flexibility.\nAccuracy: The model you select should have excellent performance and accuracy. Within this book, I have endeavored to explore and evaluate about 10 distinct open-source language models using LangChain, drawing from my personal experiences across various projects. This approach aims to provide firsthand experience to Language Learning Models (LLMs), encompassing models like Mistral, Llama2, Gemma, Flan, and GPT-2. I will systematically delve into each model with accompanying code examples for a comprehensive understanding.\nIntegration: To facilitate the integration of the model into your current systems, search for an LLM provider that provides user-friendly APIs or SDKs. As an illustration, consider Google’s Gemma model, which was unveiled in early 2024. To explore its integration with Transformers, you can refer to https://huggingface.co/google/gemma-7b?library=true . This resource demonstrates the straightforward process of integrating Gemma into a Python library.\nScalability: The model you select should be able to handle the amount of data you intend to process, especially if you require large-scale real-time responses.\nCost: Understand the pricing model, which could be influenced by factors like token quantity, API usage, or computational hours, as seen in platforms like OpenAI. In this book, the language models employed in this guide will be entirely open source. This means they are technically free to utilize, offering an alternative to services like OpenAI.\nStorage: In terms of storage, options like ElasticSearch, FAISS, or Qdrant can be used based on your specific requirements. FAISS, for example, is fast due to its GPU support but requires you to maintain your metadata information separately in some database with mapping of your FAISS ids. For long documents, it’s useful to split the document into multiple sections if your transformer model has a context length limit (such as 512 tokens), and each section corresponds to its own vector.\nAgain, I will explore the open-source language models featured in this guide, accessible at https://huggingface.co\nIn conclusion, the precise tasks you need to complete, the model’s size, its accuracy, ease of integration, scalability, cost, and the kind of semantic search you’re using will all influence your choice of LLM. It’s worthwhile to regularly visit https://huggingface.co/models to explore its playground and select a model that fits your real-world situation.\nSome Models with My Experience I have been a part of various genuine customer interactions where we delved into discussions regarding the design conversation flow and business situations. The LangChain framework serves as the backbone for crafting project designs, and I’m eager to impart some insights I have gained from these experiences. These lessons, drawn from real-world projects, offer valuable perspectives, such as collecting raw data, choosing appropriate LLMs, customizing prompt(s) and establishing a precise chain to handle communication.\nMistral 7B stands as a cutting-edge 7.3 billion parameter language model, marking a significant leap in the realm of large language models (LLMs). It has surpassed the performance of the 13 billion parameter Llama 2 model across all tasks and excels over the 34 billion parameter Llama 1 on numerous benchmarks. Undoubtedly, this model ranks among the top-tier open-source LLMs I have utilized in LangChain development projects, offering impressive performance while demanding reasonable computational resources (such as a GPU with 16GB memory like an NVIDIA RTX 4090). Its strengths lie in performance, orchestration flexibility, accuracy, competitiveness, and comparison with services like OpenAI. This book primarily showcases code examples focused on the Mistral LLM, conveniently packaged by GPT4All and Ollama for easy use. I love this model.\nFLAN-T5 is a publicly available, extensive language model designed for sequence-to-sequence tasks, suitable for commercial applications. Developed by Google researchers towards the end of 2022, this model has undergone fine-tuning across diverse tasks. The T5 model restructures different tasks into a text-to-text framework, encompassing activities like translation, linguistic evaluation, sentence comparison, and document condensation. FLAN represents “Fine-tuned LAnguage Net,” while T-5 stands for “Text-To-Text Transfer Transformer.”\nLlama2: Llama-2 is free to download, but Meta requires a register to grant us access to this model’s family with additional commercial terms. The request is made through a Meta Webpage, which can be accessed from the model homepage on Hugging Face. You can use it as the base to train your own model with fine-tuning.\nGPT-2: The GPT-2 model, a formidable transformer-based language model with 1.5 billion parameters, was trained using a dataset of 8 million web pages. Its primary training goal is to predict the next word by considering all preceding words in a text. The diverse nature of the dataset ensures that this fundamental objective covers genuine instances of numerous tasks across various fields. GPT-2 stands as a notable improvement over its forerunner, GPT, with more than ten times the parameters and trained on over ten times the data volume.\nMiniLM: You can find the details by searching sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 or sentence-transformers/all-mpnet-base-v2 at https://huggingface.co/sentence-transformers . It can be used for tasks like clustering embedding or semantic similarity search. Throughout this guide, we will utilize these models for embedding with the VectorStore.\nGemma: a series of cutting-edge open models, is derived from Gemini’s technology. With options of 2 billion and 7 billion parameters, Gemma demonstrates superior performance in language tasks and safety assessments, outperforming competitors in 11 out of 18 text-based tasks. The project prioritizes responsible deployment of LLMs to improve safety and drive innovation. Gemma’s lightweight design and open-source approach position it as a significant advancement in LLMs. We’re going to use gemma-2b model in next chapters. The model can be found at https://huggingface.co/google/gemma-2b\nt5-base-finetuned-wikiSQL: I found this one intriguing from Hugging Face and used this model to generate translate user’s instruction in text into SQL. Here’s and give a snippet of code as example: (To run the Python code provided, it is crucial to prepare a functional environment, a process that will be elaborated on in the subsequent chapter. You may find it beneficial to proceed directly to that chapter to set up the necessary environment for running the code.)\nfrom pprint import pprint from dotenv import load_dotenv load_dotenv() from langchain_community.llms import HuggingFaceHub hub_llm = HuggingFaceHub(repo_id=\"mrm8488/t5-base-finetuned-wikiSQL\") from langchain.prompts import PromptTemplate prompt = PromptTemplate( input_variables=[\"question\"], template=\"Translate English to SQL: {question}\" ) from langchain.chains import LLMChain hub_chain = LLMChain(prompt=prompt, llm=hub_llm, verbose=True) pprint(hub_chain(\"What is the average age of the respondents using a mobile device?\")) pprint(hub_chain(\"What is the median age of the respondents using a mobile device?\")) The output shows a SQL generated from t5-base-finetuned-wikiSQL model, which is fine-tuned from Google’s T5\n\u003e Entering new LLMChain chain... Prompt after formatting: Translate English to SQL: What is the median age of the respondents using a mobile device? \u003e Finished chain. {'question': 'What is the median age of the respondents using a mobile ' 'device?', 'text': 'SELECT Median age (years) FROM table WHERE Device = mobile'} After reviewing a variety of language models from numerous open-source repositories, let’s delve into understanding how to configure the typical and widely used settings of these models.",
    "description": "When selecting a language model and an embedding model in LangChain technology, there are several things to consider:\nPrimary Task: Identify the core functions of the language model (LLM), which include tasks like text generation, summarization, translation, and answering queries. A valuable resource to explore these tasks is https://huggingface.co/models covering a wide range from Multimodal to Computer Vision (CV) and Natural Language Processing (NLP). Common examples in this context involve Summarization, Text Generation, and Question Answering within NLP.",
    "tags": [],
    "title": "Select a Right Language Model",
    "uri": "/langchain_project_book/fundamentals/select_a_right_lm/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e Tools and Libraries",
    "content": "To develop LangChain applications, you will need an Integrated Development Environment (IDE). Visual Studio Code (VSCode) is one of the popular choices.\nVSCode offers several advantages that make it one of the most recommended IDEs for Python programming:\nIt is open-source. Lightweight compared to PyCharm. Built-in Python support and debugging capabilities. Boasts a large community for support. Offers numerous extensions that can significantly enhance productivity. On the other hand, PyCharm is specifically tailored for Python development and provides extensive community support. It has open-source edition too.\nHere is a list of installed extensions in VSCode in my environment on Linux, for reference:\n(gpt) [jeff@fedora huggingface]$ code --list-extensions | xargs -L 1 echo code --install-extension code --install-extension HuggingFace.huggingface-vscode code --install-extension ms-python.pylint code --install-extension ms-python.python code --install-extension ms-python.vscode-pylance code --install-extension mushan.vscode-paste-image code --install-extension shd101wyy.markdown-preview-enhanced code --install-extension streetsidesoftware.code-spell-checker code --install-extension TabNine.tabnine-vscode code --install-extension vscodevim.vim extension function huggingface-vscode LLM powered development for VSCode ms-python.pylint a static code analyser for Python ms-python.python enable Python within VSCode ms-python.vscode-pylance a language server for Python that provides advanced type checking, auto-imports, and code completions vscode-paster-image copy and paste image within markdown editor markdown-preview-enhanced one of the most prettiest preview tool for markdown editing code-spell-checker a spell checker tabnine-vscode AI coding tool vscodevim.vim enable vim in VSCode You can install the extensions listed in the script above, or you can install them in VSCode GUI.\nIf you are a fan of vim, you can enable Python IDE in vim by following the configuration guide at https://realpython.com/vim-and-python-a-match-made-in-heaven/\nEnabling a Python IDE in Vim offers the following advantages:\nIt provides a stylish and cool appearance, particularly on Unix systems (I am a Linux enthusiast, and Vim is my primary editor in Linux/Unix environments). It is robust in displaying documentation on-the-fly as you code without needing a GUI. Vim is lightweight and does not consume excessive resources. Being open-source, it aligns with the ethos of accessibility and community collaboration.",
    "description": "To develop LangChain applications, you will need an Integrated Development Environment (IDE). Visual Studio Code (VSCode) is one of the popular choices.\nVSCode offers several advantages that make it one of the most recommended IDEs for Python programming:\nIt is open-source. Lightweight compared to PyCharm. Built-in Python support and debugging capabilities. Boasts a large community for support. Offers numerous extensions that can significantly enhance productivity. On the other hand, PyCharm is specifically tailored for Python development and provides extensive community support. It has open-source edition too.",
    "tags": [],
    "title": "Install an IDE",
    "uri": "/langchain_project_book/tools_n_lib/install_ide/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook",
    "content": "This chapter provides a comprehensive guide on how to set up LangChain in a Python environment. It is essential to have a basic understanding and knowledge of Python as a prerequisite for this guide. The following steps will help you establish a robust Python environment tailored for LangChain development.",
    "description": "This chapter provides a comprehensive guide on how to set up LangChain in a Python environment. It is essential to have a basic understanding and knowledge of Python as a prerequisite for this guide. The following steps will help you establish a robust Python environment tailored for LangChain development.",
    "tags": [],
    "title": "Tools and Libraries",
    "uri": "/langchain_project_book/tools_n_lib/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e LangChain Fundamentals",
    "content": "LLM Settings When working with prompts, you can communicate directly or through an API with the LLM. A few parameters can be set to get different outcomes for your prompts.\ntemperature: In other words, results are more deterministic when the temperature is lower because the next most likely token is always selected. A higher temperature may cause more unpredictability, which promotes more varied or imaginative results. In essence, you are making the other potential tokens heavier. In order toTo encourage more factual and succinct responses, you might want to apply a lower temperature value for tasks like fact-based quality assurance. It could be useful to raise the temperature value for writing poems or other creative tasks. The impact of adjusting this value can vary significantly based on the settings of the pre-trained model. Hence, it is advisable to experiment with and fine-tune this parameter according to the specific models to align with your requirements.\ntop_k: In text generation, a language model predicts the next word by analyzing preceding words. While one common method involves choosing the word with the highest probability, known as “greedy decoding,” it can lead to repetitive and incoherent text. This is where sampling methods such as Top-K sampling offer a solution. Top-K sampling simplifies the process by restricting the selection to the K most probable next words from the vocabulary, allowing for more varied and coherent text generation.\ntop_p: Top-K sampling limits the selection to the K most probable next words, whereas Top-P sampling, also referred to as “nucleus sampling,” introduces a different approach. Rather than defining a fixed number of top candidates (K), it involves setting a probability mass (P) and sampling exclusively from the smallest subset of words with a combined probability exceeding P. max_length: By changing the max_length, you can control how many tokens the model produces. You can avoid lengthy or irrelevant responses and keep costs under control by setting a maximum length.\nmax_new_tokens: The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\nThe above are the default parameters in settings that we use for our projects. We’ll go through them in real code later in this book.\nLLM Limitations While LLMs in LangChain technology have many advantages, they also come with several limitations:\nComputational Resources: Training LLMs require significant computational resources, which can be expensive and time-consuming. Even smaller models can take days or weeks to train on powerful hardware. Data Requirement: LLMs require large amounts of diverse and high-quality data for training. Gathering such data can be challenging, and biases in the training data can lead to biased model outputs. This obstacle poses a significant challenge in my practical experience, hindering the fine-tuning of my domain-specific model and dissuading me from training my own model. Model Interpretability: LLMs are often seen as “black boxes” because their internal workings are complex and not easily understood. This makes it difficult to diagnose and fix issues when the model produces incorrect or unexpected results. It is anticipated that a growing number of open-source LLMs will be introduced within the community to offer enhanced domain-specific capabilities and greater control over data processing workflows. Adaptability: While LLMs are good at general tasks, they may not perform well in specific domains without fine-tuning. Fine-tuning itself can be tricky and requires domain-specific data. In my personal experience, every client I work with demands industry-specific solutions and robust data security measures. Addressing these requirements entails incorporating extensive additional knowledge into project design and implementation, which can pose a challenge for enterprises looking to integrate generative AI within certain industries. Ethical Concerns: LLMs can generate inappropriate or offensive content if not properly controlled. They might also inadvertently leak sensitive information if they were trained on such data. Dependency on Language: LLMs perform best on languages with a large amount of available training data, typically English. Performance might degrade for low-resource languages. A notable advantage I discovered is the ease with which LLMs handle language translation without the need for complex configurations. In my experience, we experimented with utilizing a pure English LLM to generate responses directly, rather than employing language-specific models for non-English languages and then translating as needed, all while customizing prompt instructions. Limitations in Understanding: While LLMs can generate human-like text, they actually don’t understand the content they’re generating. They can’t make logical inferences outside of their training data or handle tasks that require common sense. Environmental Impact: The energy consumption for training LLMs can be substantial, leading to a significant carbon footprint. In conclusion, while LLMs are powerful tools in LangChain technology, they do come with their own set of challenges. It’s crucial to be aware of these limitations when implementing and using these models in real case.",
    "description": "LLM Settings When working with prompts, you can communicate directly or through an API with the LLM. A few parameters can be set to get different outcomes for your prompts.\ntemperature: In other words, results are more deterministic when the temperature is lower because the next most likely token is always selected. A higher temperature may cause more unpredictability, which promotes more varied or imaginative results. In essence, you are making the other potential tokens heavier. In order toTo encourage more factual and succinct responses, you might want to apply a lower temperature value for tasks like fact-based quality assurance. It could be useful to raise the temperature value for writing poems or other creative tasks. The impact of adjusting this value can vary significantly based on the settings of the pre-trained model. Hence, it is advisable to experiment with and fine-tune this parameter according to the specific models to align with your requirements.",
    "tags": [],
    "title": "LLM Settings and Limits",
    "uri": "/langchain_project_book/fundamentals/llm_settings_n_limits/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e LangChain Fundamentals",
    "content": "The process of creating meaningful prompts or instructions for an AI system or language model is known as “prompt engineering”. The model’s responses and behaviors are greatly influenced by these cues. Prompts with a good design facilitate researchers and developers to modify the output of AI systems to accomplish desired results and enhance the model’s functionality.\nfolder INSTRUCTION { artifact \"\"\"\"Answer the question based on the context below.\\nIf the question cannot be answer using the \\ninformation provided answer with \"I don't know\".\" } folder CONTEXT { artifact \"Italian cuisine has a rich and diverse history that spans centuries \\nand has been influenced by numerous cultures. In the Roman Empire, food \\nwas a significant part of social life. Romans loved feasting and trying \\nnew dishes, and their banquets often featured complex, intricate \\nflavors that required sophisticated preparation techniques. They \\nembraced the flavors and ingredients of many of the lands they had \\nconquered, such as spices from the Middle East, fish from the \\nMediterranean, and cereals from North Africa. This fusion of diverse \\ningredients made the Empire a hot spot for culinary innovation\" } folder QUESTIONS { artifact \"What is the history of Italian cuisine?\" } folder ANSWER { artifact \"\"\"\"\"\"\" } INSTRUCTION -[hidden]- CONTEXT CONTEXT -[hidden]- QUESTIONS QUESTIONS -[hidden] ANSWER Figure 1.3: Prompt Template\nIn the context of LLMs, it consists of several key components:\nInstruction: An instruction is a specific directive given to the model, guiding its responses. It can be a statement, command, or question that dictates how the AI should process input data. For instance, instructions like “Translate this English text to French” or “Summarize the article” shape the model’s output. Crafting precise prompts significantly impacts the quality of an LLM’s results. My personal experience in LLM development reveals that output quality varies based on prompt quality, as each LLM has its preferred prompt format and template. Hence, the potential for standardizing prompts and prompt templates in metadata before adapting them to an LLM presents an innovative opportunity, such as Lepton and AutoPrompt open-source projects that I mentioned earlier. I aspire to delve deeper into this area through further research.\nContext: Context offers relevant background details to steer the model’s response, aiding in specific situational, topical, or domain-related prompts. For example, in chatbot interactions, context encompasses prior messages. In machine translation, it includes surrounding text. In practice, maintaining a completely private context ensures that similarity search outputs from a local LLM remain private and secure.\nQuestion: The question is the specific query or input data that we want the model to respond to. It’s the focus of the prompt and is typically what you want the model to process and generate a response for. The question should be clear and concise to ensure the model can understand and accurately respond to it.\nAnswer: The answer (or output, or result) is the response generated by the model based on the instruction, context, and question. The format and type of answer can be guided by an output indicator, which specifies the kind of response expected from the model. For instance, an output indicator might specify that the answer should be a list of items, a single word, or a full sentence.\nEach of these components plays a vital role in shaping the model’s response, and their effective use can greatly enhance the model’s performance on a wide range of tasks. However, it is important to note that not all components are required for every prompt, and the format of the prompt can vary depending on the task at hand.\nHere’s an example of defining prompt in ChatPromptTemplate class in LangChain. To augment the prompt by incorporating extra context, it is essential to create a prompt template. This template allows for easy customization of the prompt, as illustrated below.\nfrom langchain.prompts import ChatPromptTemplate template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. Question: {question} Context: {context} Answer: \"\"\" prompt = ChatPromptTemplate.from_template(template) print(prompt) The output will display input_variables containing context and question in PromptTemplate.\ninput_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-anwering tasks. Use the \\nfollowing pieces of retrieved context to answer the question. If you don't know\\nthe answer, just say that you don't know. Use three sentences maximum and keep\\nthe answer concise.\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n\"))] By understanding the significance of prompts and prompt templates in LangChain, we lay the foundation for exploring more advanced concepts like VectorStore and embeddings.",
    "description": "The process of creating meaningful prompts or instructions for an AI system or language model is known as “prompt engineering”. The model’s responses and behaviors are greatly influenced by these cues. Prompts with a good design facilitate researchers and developers to modify the output of AI systems to accomplish desired results and enhance the model’s functionality.\nfolder INSTRUCTION { artifact \"\"\"\"Answer the question based on the context below.\\nIf the question cannot be answer using the \\ninformation provided answer with \"I don't know\".\" } folder CONTEXT { artifact \"Italian cuisine has a rich and diverse history that spans centuries \\nand has been influenced by numerous cultures. In the Roman Empire, food \\nwas a significant part of social life. Romans loved feasting and trying \\nnew dishes, and their banquets often featured complex, intricate \\nflavors that required sophisticated preparation techniques. They \\nembraced the flavors and ingredients of many of the lands they had \\nconquered, such as spices from the Middle East, fish from the \\nMediterranean, and cereals from North Africa. This fusion of diverse \\ningredients made the Empire a hot spot for culinary innovation\" } folder QUESTIONS { artifact \"What is the history of Italian cuisine?\" } folder ANSWER { artifact \"\"\"\"\"\"\" } INSTRUCTION -[hidden]- CONTEXT CONTEXT -[hidden]- QUESTIONS QUESTIONS -[hidden] ANSWER Figure 1.3: Prompt Template",
    "tags": [],
    "title": "Thoughts on Prompt Engineering",
    "uri": "/langchain_project_book/fundamentals/thoughts_on_prompt/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e LangChain Fundamentals",
    "content": "In LangChain, a Python library designed to simplify the process of building Natural Language Processing (NLP) applications using LLMs, embeddings and VectorStore play a crucial role in enhancing the accuracy and efficiency of these applications.\nUnderstanding Embeddings In the realm of LLMs, embeddings serve as numeric depictions of words, phrases, or sentences, encapsulating their semantic meaning and context. These embeddings facilitate text representation in a machine-learning-friendly format, supporting a range of NLP endeavors. Commonly pretrained on vast text datasets, embeddings such as Word2Vec, GloVe, and FastText capture semantic connections and resemblances among words. By converting words into vectors, these embeddings streamline tasks like text analysis and generation, empowering models to comprehend and handle language proficiently by leveraging contextual cues.\nI recommend dedicating some time to review an insightful document authored by Sascha Metzger, which elaborates on tokens, vectors, and embeddings in the field of natural language processing. The document can be accessed at https://saschametzger.com/blog/what-are-tokens-vectors-and-embeddings-how-do-you-create-them .\nIn the context of an LLM with LangChain, embeddings are used to capture the “meaning” of text. The closeness of two vectors indicates the degree of correlation between them, where shorter distances imply a stronger correlation, and longer distances suggest a weaker correlation.\nHere is an example of how to create vector embeddings using the sentence_transformers library in Python: In this example, embeddings.embed_query(text) generates embeddings for the given text.\n# load embedding library from langchain_community.embeddings import HuggingFaceEmbeddings # define embedding with using sentence-transformers model from HuggingFace.co embeddings = HuggingFaceEmbeddings( model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" ) text = \"harry potter’s owl is in the castle.\" # embed the given text embed_text = embeddings.embed_query(text) # print the first 3 embedding print(embed_text [:3]) # print out the type of embeddings print(type(embed_text)) The model utilized in this instance is the paraphrase-multilingual-MiniLM from sentence-transformers, known for its resilience, efficiency, and compact size of 477MB for local storage. Despite various alternatives, this model is favored for its simplicity and effectiveness. Following the embedding procedure, the embedded data is going to be inserted into VectorStore, ensuring long-term persistence and readiness for subsequent similarity-search operations.\nUnderstanding VectorStore Understanding a VectorStore involves grasping its role in efficiently searching and comparing extensive sets of vectors, which are essential for AI applications by translating words into numerical representations to simplify sentence comparisons based on their semantic meanings.\nFunctionality of VectorStore: In LangChain, the VectorStore serves as a repository for embeddings, enabling streamlined searches based on semantic similarity. Text undergoes embedding and is then stored in the VectorStore for long-term retention, preparing it for subsequent similarity inquiries.\nVariety of VectorStore options: LangChain offers support for vector databases as its primary index type, encompassing features like document loaders, text splitters, VectorStores , and retrievers. These databases contain individual nodes alongside their respective embeddings within a VectorStore.\nBenefits of utilizing VectorStore: Integrating VectorStore in LangChain provides advantages such as efficient storage and retrieval of embeddings, facilitating quick and accurate similarity searches rooted in semantic relationships. By storing embeddings close to domain-specific datasets, seamless integration with additional metadata is enabled without external queries.\nI will be presenting a variety of open-source VectorStores. Among the most popular in the LLM domain are FAISS, Qdrant, Pinecone, and Chroma. Each of these open-source vectorstore databases will be explored through sample code within this book.\nUtilizing Embedding and VectorStore in LangChain In LangChain, embedding and VectorStore collaboratively foster the creation of intelligent agents capable of interpreting and implementing human language commands. Initially, textual data is subjected to processing and transformation into embeddings via appropriate models. Subsequently, these embeddings are deposited in a VectorStore for expeditious retrieval.\nUpon receipt of novel instructions or queries, the system may rapidly extract pertinent embeddings from the VectorStore, contrast them against the embeddings derived from the incoming command, and subsequently formulate a reply.\nBelow is an example of how this might manifest in code, employing the Sentence-Transformers library for embeddings and inserting the embeddings into FAISS open-source VectorStore, which is running in memory.\n# load embedding model from langchain_community.embeddings import HuggingFaceEmbeddings embedding = HuggingFaceEmbeddings( model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" ) # load vectorstore library from langchain_community.vectorstores import FAISS # insert the embedded data into vectorstore in memory vectorstore = FAISS.from_texts( [\"harry potter's owl is in the castle\"], embedding = embedding, ) print(vectorstore) Once the embedded data is stored in VectorStore, we will explore the process of orchestrating retrieval and chaining to obtain responses from the LLM.",
    "description": "In LangChain, a Python library designed to simplify the process of building Natural Language Processing (NLP) applications using LLMs, embeddings and VectorStore play a crucial role in enhancing the accuracy and efficiency of these applications.\nUnderstanding Embeddings In the realm of LLMs, embeddings serve as numeric depictions of words, phrases, or sentences, encapsulating their semantic meaning and context. These embeddings facilitate text representation in a machine-learning-friendly format, supporting a range of NLP endeavors. Commonly pretrained on vast text datasets, embeddings such as Word2Vec, GloVe, and FastText capture semantic connections and resemblances among words. By converting words into vectors, these embeddings streamline tasks like text analysis and generation, empowering models to comprehend and handle language proficiently by leveraging contextual cues.",
    "tags": [],
    "title": "Embeddings and VectorStore",
    "uri": "/langchain_project_book/fundamentals/embedding_n_vectorstore/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e LangChain Fundamentals",
    "content": "LangChain can easily orchestrate interactions with language models, chain together various components, and incorporate resources like databases and APIs. We will examine two fundamental ideas in LangChain in this chapter: Chain and Retriever.\nUnderstanding Chains LangChain relies heavily on chains. The core of LangChain’s operation is these logical relationships among one or more LLMs. Depending on the requirements and LLMs involved, chains might be simple or complex. An LLM model, an output parser that is optional, and a PromptTemplate are all part of its organized configuration. The LLMChain in this configuration takes in a variety of input parameters. It converts these inputs into a logical prompt by using the PromptTemplate. The model is then fed this polished cue. Following receipt of the output, the LLMChain formats and further refines the result into its most useable form using the OutputParser, if one is supplied.\nfolder chain { artifact input_variables artifact PromptTemplate artifact LLM } input_variables -\u003e PromptTemplate PromptTemplate -\u003e LLM folder prompt { artifact \"prompt = PromptTemplate(\\n input_variables=[\"city\"],\\n template=\"Describe a perfect day in {city}?\"\\n)\" } folder chain.invoke { artifact \"chain = LLMChain(\\n llm=llm, prompt=prompt\\n)\\nprint(chain.invoke(\"Paris\"))\" } chain -[hidden]- prompt prompt -[hidden] chain.invoke To demonstrate LangChain’s capability for creating simple chains, here is an instance utilizing the HuggingFaceEndpoint class.\nFirst the code imports PromptTemplate from langchain.prompts and defines a prompt using the template “Describe a perfect day in {city}?” where {city} is a variable placeholder. It imports an LLM from Hugging Face using HuggingFaceEndpoint, specifying the model repository id as mistralai/Mistral-7B-Instruct and setting model parameters like temperature and max_new_token. Then the code imports LLMChain from langchain.chains and creates a chain (llmchain) by combining the selected LLM (mistral) and the defined prompt. Finally, it invokes the chain with a query “Paris” using llmchain.invoke(\"Paris\") and prints out the result.\n(TL;DR\nConsider reading the following chapter to efficiently configure your environment for running the provided code examples.)\nfrom langchain.prompts import PromptTemplate prompt = PromptTemplate( input_variables=[\"city\"], template=\"Describe a perfect day in {city}?\") from langchain_community.llms import HuggingFaceEndpoint repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\" llm = HuggingFaceEndpoint( repo_id=repo_id, max_new_tokens=250, temperature=0.5 ) from langchain.chains import LLMChain llmchain = LLMChain(llm=llm, prompt=prompt, verbose=True) print(llmchain.invoke(\"Paris\")) In summary, this code sets up a scenario where an LLM model is prompted to describe a perfect day in Paris based on the defined template and model settings. You see the output varies while changing the setting of temperature. I enabled verbose mode to showcase a more comprehensive display of detailed steps and information in the output.\nToken is valid (permission: read). Your token has been saved to /home/jeff/.cache/huggingface/token Login successful \u003e Entering new LLMChain chain... Prompt after formatting: Describe a perfect day in Paris? \u003e Finished chain. {'city': 'Paris', 'text': \"\\n\\nA perfect day in Paris would begin with waking up early in a charming apartment located in the heart of the city. After a quick breakfast at a local bakery, I would head out to explore the beautiful Montmartre district, taking in the breathtaking views of the city from the top of the Sacré-Cœur.\\n\\nNext, I would stroll through the picturesque streets of Le Marais, stopping to admire the historic architecture and browse the trendy boutiques. I would then make my way to the Louvre Museum to spend the afternoon exploring the countless works of art housed within its walls.\\n\\nAs the sun begins to set, I would take a leisurely boat ride along the Seine River, taking in the stunning views of the city's iconic landmarks such as the Eiffel Tower and Notre-Dame Cathedral. I would then enjoy a delicious dinner at a quintessential Parisian bistro, accompanied by a glass of fine French wine.\\n\\nThe evening would be spent exploring the vibrant nightlife of the city, perhaps catching a cabaret show or dancing the night away at a trendy club. Finally, I would\"} Next, we will delve into employing a retriever to fetch data from an established VectorStore. This process ensures that the retrieved data originates exclusively from the designated VectorStore source. When both the LLM and VectorStore reside within a private network, all information involved in retrieval and result generation remains secure and private. Such practices effectively address security and privacy considerations within enterprise environments.\nUnderstanding Retrievers In LangChain, a retriever acts as a pivotal component responsible for fetching relevant information from a knowledge base or content source in response to user queries.\nRAG, short for Retrieval Augmented Generation, is a method employed to integrate additional, often confidential or current data into the knowledge of LLMs. While LLMs demonstrate proficiency in analyzing diverse subjects, their expertise is limited to publicly accessible data present during their training phase. To enable AI applications to reason over private or post-training data effectively, it is imperative to supply the model with precise information to expand its knowledge. RAG entails retrieving pertinent data and seamlessly incorporating it into the model prompt.\nUtilizing Chains and Retrievers in LangChain Combining chains and retrievers in LangChain allows for the creation of intricate workflows that process user input, obtain pertinent data, and produce output.\nWhen a new question is received, the chain can be utilized to process it, produce a prompt, and use the retriever to get relevant data from the knowledge base. A response can then be produced using the information that was retrieved.\nThe following diagram outlines the intricate process:\nInitially, load the document, then split and embed it in a vectorstore, which serves as the retriever for subsequent retrieval in the RAG step.\nUtilize the HuggingFacePipeline to load the Transformers language model for embedding.\nMerge the steps with a customizable prompt to create a chain structure, typically followed by inclusion in the chain using the RunnablePassthrough class to generate the answer.\nfolder documents { artifact document_loaders artifact text_splitter } folder retrievers { artifact embeddings artifact vectorstore artifact retriever } folder rag_chain { artifact RunnablePassthrough folder prompt_template { artifact prompt } folder llm { artifact HuggingFaceEndpoint } } file answer document_loaders -\u003e text_splitter text_splitter --\u003e embeddings embeddings -\u003e vectorstore vectorstore -\u003e retriever retriever -[hidden]- rag_chain retriever --\u003e RunnablePassthrough RunnablePassthrough -\u003e prompt prompt --\u003e HuggingFaceEndpoint HuggingFaceEndpoint -\u003e answer In conclusion, LangChain’s chains and retrievers offer an adaptable and potent approach to developing LLM-based applications. These tools enable developers to design intricate processes that manage user input, obtain relevant data, and provide output.\nHere is an example of how to use a retriever in LangChain. In this example, the code performs the following steps:\nUtilizes a sentence-transformers embedding model to embed data fetched from a website, subsequently inserting the embedded data into a FAISS VectorStore.\nRetrieves relevant data from the retriever (the VectorStore) and processes it through a RAG chain using the RunnablePassthrough library to reach the LLM, incorporating a predefined prompt, and generates the answer.\nLet’s go through it step by step.\nLoad a document from the web, through WebBaseLoader class, split it into small chunks\nfrom langchain_community.document_loaders import WebBaseLoader loader = WebBaseLoader(\"https://en.wikisource.org/wiki/Hans_Andersen%27s_Fairy_Tales/The_Emperor%27s_New_Clothes\") document = loader.load() from langchain.text_splitter import RecursiveCharacterTextSplitter splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20) chunks = splitter.split_documents(document) Load the embedding model.\nfrom langchain_community.embeddings import HuggingFaceEmbeddings embedding = HuggingFaceEmbeddings( model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\") The from_documents method is used to initialize the FAISS VectorStore with the given chunks and embedding. Subsequently, the code converts this VectorStore into a retriever object using the as_retriever() method.\nfrom langchain_community.vectorstores import FAISS vectorstore = FAISS.from_documents(chunks, embedding) retriever = vectorstore.as_retriever() Define prompt with ChatPromptTemplate method.\nfrom langchain.prompts import ChatPromptTemplate template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. Question: {question} Context: {context} Answer: \"\"\" prompt = ChatPromptTemplate.from_template(template) Use the google/flan-t5-xxl model through the Hugging Face platform’s HuggingFaceEndpoint method remotely. Specify the model’s keyword arguments to customize settings such as temperature and max_new_token. Utilizing HuggingFaceEndpoint allows you to leverage the computational resources provided by Hugging Face, eliminating the need for substantial local computing capacity. You may also experiment with Mistral’s model to observe variations in generation quality.\nfrom langchain_community.llms import HuggingFaceEndpoint repo_id=\"google/flan-t5-xxl\" # repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\" llm = HuggingFaceEndpoint( repo_id=repo_id, max_new_tokens=250, temperature=0.5) The RAG chain combines the context retrieved from the retriever, the question, the prompt, and the language model to produce an answer using the RunnablePassthrough method. This chain is both straightforward and engaging in its approach.\nfrom langchain.schema.runnable import RunnablePassthrough from langchain.schema.output_parser import StrOutputParser rag_chain = ( {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser() ) query = \"What is this story telling about?\" print(rag_chain.invoke(query)) The output will display:\nThe emperor's new clothes The code presented above is straight-forward yet provides a comprehensive, direct, and lucid sequence to manage the entire process:\nExtracting a document from the web, splitting it into smaller parts, and storing them in a VectorStore.\nEstablishing a retriever with the VectorStore.\nTailoring a prompt.\nEmploying an LLM from HuggingFaceEndpoint method.\nIntegrating all the components mentioned above into a coherent chain.",
    "description": "LangChain can easily orchestrate interactions with language models, chain together various components, and incorporate resources like databases and APIs. We will examine two fundamental ideas in LangChain in this chapter: Chain and Retriever.\nUnderstanding Chains LangChain relies heavily on chains. The core of LangChain’s operation is these logical relationships among one or more LLMs. Depending on the requirements and LLMs involved, chains might be simple or complex. An LLM model, an output parser that is optional, and a PromptTemplate are all part of its organized configuration. The LLMChain in this configuration takes in a variety of input parameters. It converts these inputs into a logical prompt by using the PromptTemplate. The model is then fed this polished cue. Following receipt of the output, the LLMChain formats and further refines the result into its most useable form using the OutputParser, if one is supplied.",
    "tags": [],
    "title": "Chains and Retriever",
    "uri": "/langchain_project_book/fundamentals/chains_n_retriever/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e LangChain Fundamentals",
    "content": "Understanding LangChain’s API offers the following advantages:\nFacilitating application development. Assisting in debugging by enabling verbose mode to provide more detailed traces. API Document in LangChain LangChain provides an API that enables developers to easily interact with LLM and other computational resources or knowledge sources. This makes it easier to build applications such as question answering systems, chatbots, and intelligent agents.\nI advise you to often check out the latest LangChain API reference, where you may examine incredibly thorough API logic at https://api.python.langchain.com/en/latest/ .\nDebugging in LangChain Debugging is an important aspect of building applications with LangChain. There are several options that you can enable debug and/ or verbose mode in code.\nThe functions set_debug(True) and set_verbose(True) are utilized to activate debugging and verbose logging, respectively. They allow for logging intermediate stages of components and offering detailed outputs, aiding in the debugging process by printing detailed chains step by step.\nfrom langchain.globals import set_debug, set_verbose set_debug(True) set_verbose(True) CallbackHandlers are objects that implement the CallbackHandler interface, which has a method for each event that can be subscribed to. The CallbackManager will call the appropriate method on each handler when the event is triggered. (credit: this information is from https://python.langchain.com/docs/modules/callbacks/). Here’s an example within RetrievalQA function:\nfrom langchain.callbacks import StdOutCallbackHandler retrievalQA = RetrievalQA.from_chain_type( llm=llm, retriever = VectorStoreRetriever(vectorstore=vectorstore), chain_type=\"stuff\", chain_type_kwargs={\"prompt\": PROMPT}, callbacks=[StdOutCallbackHandler()], ) Another one is ConsoleCallbackHandler, which is like StdOutCallbackHandler. Here’s an example which is very straightforward\n# Define an LLM from langchain_community.llms import HuggingFaceEndpoint repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\" llm = HuggingFaceEndpoint( repo_id=repo_id, max_new_tokens=250, temperature=0.5) # Define prompt from langchain.prompts import ChatPromptTemplate prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") # Load output parser for taking output of an LLM from langchain.schema.output_parser import StrOutputParser output_parser = StrOutputParser() chain = prompt | llm| output_parser # Enable callbacks for logging from langchain.callbacks.tracers import ConsoleCallbackHandler chain.invoke({\"topic\": \"Tintin\"}, config={'callbacks': [ConsoleCallbackHandler()]}) The result will showcase the output in a detailed step-by-step chain format.\nThe debugging features within LangChain simplify the process of pinpointing and resolving issues within your application. Mastering these tools enables you to develop more resilient and effective applications using LangChain. These tools also allow for the detailed printing of each chain step, providing a clear view of how data is processed throughout the workflow.",
    "description": "Understanding LangChain’s API offers the following advantages:\nFacilitating application development. Assisting in debugging by enabling verbose mode to provide more detailed traces. API Document in LangChain LangChain provides an API that enables developers to easily interact with LLM and other computational resources or knowledge sources. This makes it easier to build applications such as question answering systems, chatbots, and intelligent agents.\nI advise you to often check out the latest LangChain API reference, where you may examine incredibly thorough API logic at https://api.python.langchain.com/en/latest/ .",
    "tags": [],
    "title": "Document and Debugging",
    "uri": "/langchain_project_book/fundamentals/document_n_debugging/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook \u003e LangChain Fundamentals",
    "content": "Throughout this chapter, we have acquired proficiency in the following key areas:\nLangChain Architecture and Workflow: Grasping the structure and operational flow of LangChain for efficient application development. Models: Selecting appropriate language and embedding models for content embedding and generation. Prompt and Prompt Template: Introducing the use of prompts and templates to guide interactions with models, providing various examples to demonstrate their impact on the chain’s outcomes. Chain: Comprehending the sequential procedures within the system to uphold a coherent data operation flow and acknowledging the chain’s role in the comprehensive retrieval-augmented generation (RAG) process. For instance, retrieving relevant data from a retriever (vector store), combining it with a template, and forwarding it to an LLM for content generation. Debug and Verbose: Activating debugging and verbose modes to facilitate error identification and detailed logging for efficient troubleshooting. Enabling debugging and verbose modes for effective error detection and comprehensive logging to streamline issue resolution processes. By understanding the skills presented in the “LangChain Fundamentals” chapter, readers will be well-prepared to engage in application development using LangChain and its associated technologies.",
    "description": "Throughout this chapter, we have acquired proficiency in the following key areas:\nLangChain Architecture and Workflow: Grasping the structure and operational flow of LangChain for efficient application development. Models: Selecting appropriate language and embedding models for content embedding and generation. Prompt and Prompt Template: Introducing the use of prompts and templates to guide interactions with models, providing various examples to demonstrate their impact on the chain’s outcomes. Chain: Comprehending the sequential procedures within the system to uphold a coherent data operation flow and acknowledging the chain’s role in the comprehensive retrieval-augmented generation (RAG) process. For instance, retrieving relevant data from a retriever (vector store), combining it with a template, and forwarding it to an LLM for content generation. Debug and Verbose: Activating debugging and verbose modes to facilitate error identification and detailed logging for efficient troubleshooting. Enabling debugging and verbose modes for effective error detection and comprehensive logging to streamline issue resolution processes. By understanding the skills presented in the “LangChain Fundamentals” chapter, readers will be well-prepared to engage in application development using LangChain and its associated technologies.",
    "tags": [],
    "title": "Summary",
    "uri": "/langchain_project_book/fundamentals/summary/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Introduction My Journey with LangChain In late 2022, I embarked on an exciting journey with LangChain while leading a development team. This coincided with OpenAI’s significant boost in popularity, marking a pivotal moment in AI technology.\nDiscovery and Adoption After several months of exploration, our team identified LangChain as the ideal API standard and base platform for our projects. What drew us to LangChain was its comprehensive suite of features:\nConfiguring vector databases Orchestrating multiple popular embeddings and large language models from open source community Facilitating complex workflows with agents These capabilities made LangChain an indispensable tool for our development needs.\nWriting Experience During this period, I received an invitation from a renowned UK publisher to author a book about LangChain design and project implementation. Leveraging my hands-on experience, I dedicated nearly six months to crafting this comprehensive guide.\nUnfortunately, despite the effort invested, the book project was ultimately terminated. The reasons cited were a lack of a clear value proposition and insufficient trust in the communication process.\nOpen-Source Initiative Undeterred by this setback, I decided to take a different approach. Recognizing the potential benefits of sharing knowledge within the developer community, I chose to publish my work as open-source material.\nThis decision aligns with the spirit of collaborative innovation that drives many successful tech projects. By making my experiences and insights freely available, I hope to contribute meaningfully to the LangChain ecosystem and support fellow developers in their own journeys.\nCall to Action I invite you to explore these resources and share your thoughts. Your feedback and comments are invaluable in refining and expanding this body of knowledge. Together, we can create a richer understanding of LangChain and its applications.\nFeel free to engage in discussions, ask questions, or suggest improvements. Let’s foster a vibrant community around LangChain and push the boundaries of what’s possible with this powerful tool!",
    "description": "Introduction My Journey with LangChain In late 2022, I embarked on an exciting journey with LangChain while leading a development team. This coincided with OpenAI’s significant boost in popularity, marking a pivotal moment in AI technology.\nDiscovery and Adoption After several months of exploration, our team identified LangChain as the ideal API standard and base platform for our projects. What drew us to LangChain was its comprehensive suite of features:",
    "tags": [],
    "title": "LangChain Project Handbook",
    "uri": "/langchain_project_book/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/langchain_project_book/categories/index.html"
  },
  {
    "breadcrumb": "LangChain Project Handbook",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/langchain_project_book/tags/index.html"
  }
]
